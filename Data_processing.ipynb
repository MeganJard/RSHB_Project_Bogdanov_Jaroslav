{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXEHcFe50Xw6kFA32/rxql",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeganJard/RSHB_Project_Bogdanov_Jaroslav/blob/main/Data_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2"
      ],
      "metadata": {
        "id": "iOQRb63tYOQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5KB-El9qQyUs"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "import pymorphy2\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\") # стоп слова\n",
        "nltk.download('wordnet') # проводит лемматизацию\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_ru')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "lemmatize = nltk.WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "e_Aw0dlYRbIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "towar_uslugi = {}\n",
        "uslugi_towar = {}\n",
        "all_uslugi = set()\n",
        "with open('/content/materials_fully.json') as json_file:\n",
        "    materials_data = json.load(json_file)\n",
        "with open('/content/stroyka_fully.json') as json_file:\n",
        "    stroyka_data = json.load(json_file)\n",
        "with open('/content/remont_fully.json') as json_file:\n",
        "    stroyka_data |= json.load(json_file)\n",
        "with open(r'/content/Соответствие категорий товаров и услуг.csv', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        line = line.replace('\\n', '')\n",
        "        towar = line.split(';')[0]\n",
        "        uslugi = line.split(';')[1].split(', ')\n",
        "        towar_uslugi[towar] = uslugi\n",
        "\n",
        "for towar in towar_uslugi:\n",
        "    for usluga in towar_uslugi[towar]:\n",
        "        all_uslugi.add(usluga)\n",
        "for usluga in list(all_uslugi):\n",
        "    uslugi_towar[usluga] = []\n",
        "    for towar in towar_uslugi:\n",
        "        if usluga in towar_uslugi[towar]:\n",
        "            uslugi_towar[usluga].append(towar)\n"
      ],
      "metadata": {
        "id": "ZSh-xaUSRCjo"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_stopwords = open('/content/стопслова.txt').read().replace(' ', '').replace('\\t', '').split('\\n')\n"
      ],
      "metadata": {
        "id": "H0SANfFu4wxs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens(input):\n",
        "  input = input.lower()\n",
        "  regular = r'[\\*+\\#+\\№\\\"\\-+\\+\\=+\\?+\\&\\^\\.+\\;\\,+\\>+\\(\\)\\/+\\:\\\\+\\–+]'\n",
        "  regular_url = r'(http\\S+)|(www\\S+)|([\\w\\d]+www\\S+)|([\\w\\d]+http\\S+)'\n",
        "  input = re.sub(regular, '', input)\n",
        "  input = re.sub(regular_url, r'URL', input)\n",
        "  input = re.sub(r'(\\d+\\s\\d+)|(\\d+)',' NUM ', input)\n",
        "  input = re.sub(r'\\s+', ' ', input)\n",
        "  text = nltk.word_tokenize(input, language='russian')\n",
        "  text = [lemmatize.lemmatize(word) for word in text]\n",
        "  filtered_words = [word for word in text if morph.parse(word)[0].normal_form not in stopwords.words('russian')+custom_stopwords]\n",
        "  tagged = nltk.pos_tag(filtered_words, lang='rus')\n",
        "  for i in range(len(tagged)):\n",
        "    if tagged[i][1] in ['NONLEX', 'INTJ', 'V', 'ADV']:\n",
        "      tagged[i] = ''\n",
        "  tagged = list(filter(('').__ne__, tagged))\n",
        "  for i in range(len(tagged)):\n",
        "    tagged[i] = morph.parse(tagged[i][0])[0].normal_form\n",
        "\n",
        "  return tagged"
      ],
      "metadata": {
        "id": "vdt8SS0iZD59"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stroyka_dict = {}\n",
        "materials_dict = {}\n",
        "for item in stroyka_data:\n",
        "  item_dict = {}\n",
        "  item_dict['url'] = item.split('~')[2]\n",
        "  item_dict['name'] = list(set(get_tokens(item.split('~')[0])))\n",
        "  item_dict['opis'] = list(set(get_tokens(stroyka_data[item])))\n",
        "  item_dict['real_name'] = item.split('~')[0]\n",
        "  item_dict['categ'] = stroyka_data[item].split('\\n')[0]\n",
        "  stroyka_dict[item.split('~')[1]] = item_dict\n",
        "for item in materials_data:\n",
        "  item_dict = {}\n",
        "  item_dict['url'] = item.split('~')[2]\n",
        "  item_dict['name'] = list(set(get_tokens(item.split('~')[0])))\n",
        "  item_dict['real_name'] = item.split('~')[0]\n",
        "  item_dict['opis'] = list(set(get_tokens(materials_data[item])))\n",
        "  item_dict['categ'] = materials_data[item].split('\\n')[0]\n",
        "  materials_dict[item.split('~')[1]] = item_dict"
      ],
      "metadata": {
        "id": "y9NL7Yx_cEc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#теперь посчитаем подходящие материалы для каждой услуги\n",
        "answer = dict()\n",
        "\n",
        "for usluga in stroyka_dict:\n",
        "  answer[usluga] = []\n",
        "  for material in materials_dict:\n",
        "    intersection_opis = set(stroyka_dict[usluga]['opis'])&set(materials_dict[material]['opis'])\n",
        "    intersection_title = set(stroyka_dict[usluga]['opis'])&set(materials_dict[material]['name'])\n",
        "    if len(materials_dict[material]['opis']) and len(stroyka_dict[usluga]['opis']):\n",
        "        try:\n",
        "          if not (materials_dict[material]['categ'] in uslugi_towar[stroyka_dict[usluga]['categ']]):\n",
        "            continue\n",
        "        except Exception as e:\n",
        "          all_probs.add(materials_dict[material]['categ'])\n",
        "          continue\n",
        "        answer[usluga].append({material: len(intersection_opis)/((len(materials_dict[material]['opis'])+len(stroyka_dict[usluga]['opis'])))+2*len(intersection_title)/(len(stroyka_dict[usluga]['opis']))})\n",
        "  answer[usluga] = sorted(answer[usluga], key=lambda x: list(x.values())[0], reverse=True)\n",
        "  answer_with_words = dict()\n",
        "  for title in answer:\n",
        "    buffer = []\n",
        "    for i in range(min(5, len(answer[title]))):\n",
        "      buffer.append([materials_dict[list(answer[title][i].keys())[0]]['real_name'], materials_dict[list(answer[title][i].keys())[0]]['url']])\n",
        "    answer_with_words[stroyka_dict[title]['real_name']+'~~'+stroyka_dict[title]['url']] = buffer"
      ],
      "metadata": {
        "id": "_BQleaLyFcjo"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/uslugi_to_materials.json', 'w', encoding='utf-8') as fp:\n",
        "    json.dump(answer_with_words, fp, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "MHLkOOTu9BQ4"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Теперь посчитаем подходящие услуги для каждого материала\n",
        "answer = dict()\n",
        "all_probs = set()\n",
        "for material in materials_dict:\n",
        "  answer[material] = []\n",
        "  for usluga in stroyka_dict:\n",
        "    intersection_opis = set(stroyka_dict[usluga]['opis'])&set(materials_dict[material]['opis'])\n",
        "    intersection_title = set(stroyka_dict[usluga]['name'])&set(materials_dict[material]['opis'])\n",
        "    try:\n",
        "      if len(materials_dict[material]['opis']) and len(stroyka_dict[usluga]['opis']):\n",
        "        try:\n",
        "\n",
        "          if not (stroyka_dict[usluga]['categ'] in towar_uslugi[materials_dict[material]['categ']]):\n",
        "            continue\n",
        "        except Exception as e:\n",
        "          all_probs.add(materials_dict[material]['categ'])\n",
        "          continue\n",
        "        answer[material].append({usluga: len(intersection_opis)/((len(materials_dict[material]['opis'])+len(stroyka_dict[usluga]['opis'])))+2*len(intersection_title)/(len(materials_dict[material]['opis']))})\n",
        "    except Exception as e:\n",
        "      print('ERROR', e)\n",
        "  answer[material] = sorted(answer[material], key=lambda x: list(x.values())[0], reverse=True)\n",
        "  answer_with_words = dict()\n",
        "  for title in answer:\n",
        "    buffer = []\n",
        "    for i in range(min(5, len(answer[title]))):\n",
        "      buffer.append([stroyka_dict[list(answer[title][i].keys())[0]]['real_name'], stroyka_dict[list(answer[title][i].keys())[0]]['url']])\n",
        "    answer_with_words[materials_dict[title]['real_name']+'~~'+materials_dict[title]['url']] = buffer"
      ],
      "metadata": {
        "id": "6JJ-cuBJaGJX"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/materials_to_uslugi.json', 'w', encoding='utf-8') as fp:\n",
        "    json.dump(answer_with_words, fp, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "O3i_ZiZ5KJTQ"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#теперь можно проанализировать тэги, чтобы пополнить библиотеку стоп-слов\n",
        "stop_words = dict()\n",
        "all_tags_to_freq = dict()\n",
        "for material in stroyka_dict:\n",
        "  for tag in stroyka_dict[material]['name']:\n",
        "    if tag in all_tags_to_freq:\n",
        "      all_tags_to_freq[tag] += 1\n",
        "    else:\n",
        "      all_tags_to_freq[tag] = 1\n",
        "  for tag in stroyka_dict[material]['opis']:\n",
        "    if tag in all_tags_to_freq:\n",
        "      all_tags_to_freq[tag] += 1\n",
        "    else:\n",
        "      all_tags_to_freq[tag] = 1"
      ],
      "metadata": {
        "id": "HjXd2MsmuLRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_tags = sorted(all_tags_to_freq, key=lambda x: all_tags_to_freq[x], reverse=True)\n",
        "for i in all_tags:\n",
        "  print(i, all_tags_to_freq[i])"
      ],
      "metadata": {
        "id": "ttgK23Ftv6nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6_BTdYS44uWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_with_words"
      ],
      "metadata": {
        "id": "GrTZzQqj5GMG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}